{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "ROOT_URL = \"https://www.latimes.com\"\n",
    "MAX_PAGES = 20000 \n",
    "CRAWL_DELAY = 1  \n",
    "\n",
    "fetch_data = []\n",
    "visit_data = []\n",
    "url_data = []\n",
    "visited_urls = set()\n",
    "\n",
    "filters = re.compile(r'.*\\.(css|js|json|xml|zip|gz|mp3|mp4|ico|png|jpg|jpeg|svg)$')\n",
    "\n",
    "def fetch_page(url, retries=3):\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            res = requests.get(url, timeout=5, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            fetch_data.append([url, res.status_code])\n",
    "            return res\n",
    "        except requests.RequestException:\n",
    "            time.sleep(1)  \n",
    "    fetch_data.append([url, 'FAILED'])\n",
    "    return None\n",
    "\n",
    "def parse_page(response):\n",
    "    content_type = response.headers.get('Content-Type', '').split(';')[0]\n",
    "    size = len(response.content)\n",
    "    out_links = set()\n",
    "\n",
    "    if 'text/html' in content_type:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            absolute_link = urljoin(ROOT_URL, link['href'])\n",
    "            if absolute_link.startswith(ROOT_URL) and not filters.match(absolute_link):\n",
    "                if absolute_link not in visited_urls:\n",
    "                    out_links.add(absolute_link)\n",
    "                url_data.append([absolute_link, 'OK' if absolute_link.startswith(ROOT_URL) else 'N_OK'])\n",
    "\n",
    "    visit_data.append([response.url, size, len(out_links), content_type])\n",
    "    return out_links\n",
    "\n",
    "def crawl(url, max_pages):\n",
    "    pages_visited = 0\n",
    "    to_visit = [url]\n",
    "\n",
    "    while to_visit and pages_visited < max_pages:\n",
    "        current_url = to_visit.pop(0)\n",
    "        if current_url in visited_urls:\n",
    "            continue\n",
    "\n",
    "        response = fetch_page(current_url)\n",
    "        if response and response.status_code == 200:\n",
    "            new_links = parse_page(response)\n",
    "            to_visit.extend(new_links)\n",
    "\n",
    "        visited_urls.add(current_url)\n",
    "        pages_visited += 1\n",
    "\n",
    "        if pages_visited % 1000 == 0:\n",
    "            save_progress()\n",
    "\n",
    "        time.sleep(CRAWL_DELAY) \n",
    "\n",
    "def save():\n",
    "    write_csv('fetch_LATimes.csv', fetch_data, ['URL', 'Status'])\n",
    "    write_csv('visit_LATimes.csv', visit_data, ['URL', 'Size(Bytes)', 'OutLinks', 'ContentType'])\n",
    "    write_csv('urls_LATimes.csv', url_data, ['URL', 'Status'])\n",
    "\n",
    "def write_csv(file_name, data, headers):\n",
    "    with open(file_name, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(headers)\n",
    "        writer.writerows(data)\n",
    "\n",
    "crawl(ROOT_URL, MAX_PAGES)\n",
    "save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outgoing URLs:\n",
      "================\n",
      "Total URLs extracted: 3548799\n",
      "Total unique URLs extracted: 164700\n",
      "Unique URLs within News Site: 164700\n",
      "Unique URLs outside News Site: 0\n",
      "\n",
      "Fetch Statistics:\n",
      "================\n",
      "Fetches attempted: 20000\n",
      "Fetches succeeded: 18841\n",
      "Fetches failed or aborted: 1159\n",
      "\n",
      "Status Codes:\n",
      "=============\n",
      "200 OK: 18841\n",
      "404 Not Found: 1143\n",
      "FAILED Unknown Status: 8\n",
      "429 Unknown Status: 4\n",
      "403 Forbidden: 2\n",
      "999 Unknown Status: 2\n",
      "\n",
      "File Sizes:\n",
      "=============\n",
      "< 1KB: 1\n",
      "1KB ~ <10KB: 5\n",
      "10KB ~ <100KB: 160\n",
      "100KB ~ <1MB: 18240\n",
      ">= 1MB: 435\n",
      "\n",
      "Content Types:\n",
      "=============\n",
      "text/html    18840\n",
      "Name: ContentType, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "urls_df = pd.read_csv('urls_LATimes.csv')\n",
    "unique_within = urls_df[urls_df['Status'].str.strip().str.upper() == 'OK']['URL'].nunique()\n",
    "unique_outside = urls_df[urls_df['Status'].str.strip().str.upper() == 'N_OK']['URL'].nunique()\n",
    "total_unique_urls = urls_df['URL'].nunique()\n",
    "total_urls_extracted = len(urls_df)\n",
    "\n",
    "print(\"\\nOutgoing URLs:\")\n",
    "print(\"================\")\n",
    "print(f\"Total URLs extracted: {total_urls_extracted}\")\n",
    "print(f\"Total unique URLs extracted: {total_unique_urls}\")\n",
    "print(f\"Unique URLs within News Site: {unique_within}\")\n",
    "print(f\"Unique URLs outside News Site: {unique_outside}\")\n",
    "\n",
    "fetch_df = pd.read_csv('fetch_LATimes.csv')\n",
    "total_fetches_attempted = len(fetch_df)\n",
    "fetches_succeeded = len(fetch_df[fetch_df['Status'] == '200'])\n",
    "fetches_failed_or_aborted = total_fetches_attempted - fetches_succeeded\n",
    "\n",
    "print(\"\\nFetch Statistics:\")\n",
    "print(\"================\")\n",
    "print(f\"Fetches attempted: {total_fetches_attempted}\")\n",
    "print(f\"Fetches succeeded: {fetches_succeeded}\")\n",
    "print(f\"Fetches failed or aborted: {fetches_failed_or_aborted}\")\n",
    "\n",
    "status_counts = fetch_df['Status'].value_counts()\n",
    "print(\"\\nStatus Codes:\")\n",
    "print(\"=============\")\n",
    "status_messages = {\n",
    "    '200': 'OK',\n",
    "    '403': 'Forbidden',\n",
    "    '404': 'Not Found',\n",
    "    '500': 'Internal Server Error',\n",
    "    '405': 'Method Not Allowed'\n",
    "}\n",
    "\n",
    "for status, count in status_counts.items():\n",
    "    message = status_messages.get(status, 'Unknown Status')\n",
    "    print(f\"{status} {message}: {count}\")\n",
    "\n",
    "visit_df = pd.read_csv('visit_LATimes.csv')\n",
    "file_size_buckets = {\n",
    "    '< 1KB': len(visit_df[visit_df['Size(Bytes)'] < 1024]),\n",
    "    '1KB ~ <10KB': len(visit_df[(visit_df['Size(Bytes)'] >= 1024) & (visit_df['Size(Bytes)'] < 10240)]),\n",
    "    '10KB ~ <100KB': len(visit_df[(visit_df['Size(Bytes)'] >= 10240) & (visit_df['Size(Bytes)'] < 102400)]),\n",
    "    '100KB ~ <1MB': len(visit_df[(visit_df['Size(Bytes)'] >= 102400) & (visit_df['Size(Bytes)'] < 1048576)]),\n",
    "    '>= 1MB': len(visit_df[visit_df['Size(Bytes)'] >= 1048576])\n",
    "}\n",
    "\n",
    "print(\"\\nFile Sizes:\")\n",
    "print(\"=============\")\n",
    "for size_range, count in file_size_buckets.items():\n",
    "    print(f\"{size_range}: {count}\")\n",
    "\n",
    "content_type_stats = visit_df['ContentType'].value_counts()\n",
    "print(\"\\nContent Types:\")\n",
    "print(\"=============\")\n",
    "print(content_type_stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
